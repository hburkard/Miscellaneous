{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtmBknqgs1iu"
   },
   "source": [
    "#Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wq6v7uZrxyc3",
    "outputId": "5a5d7056-2c2c-4d6b-a02b-6cbab023a45b"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-d6c43e242201>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-d6c43e242201>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    conda install numpy\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "conda install numpy\n",
    "conda install -c intel mkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yS4kuWxP25Dk"
   },
   "outputs": [],
   "source": [
    "# Load Tweets\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 20)\n",
    "\n",
    "trumpTweets = pd.read_json('trump.json', lines=True)\n",
    "\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oG7FYntphx1J"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trumpTweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b7901a31f4bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Filter the dataframes to show id, content and date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrumpTweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrumpTweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trumpTweets' is not defined"
     ]
    }
   ],
   "source": [
    "# Filter the dataframes to show id, content and date\n",
    "trumpTweets = trumpTweets.filter(['id','content','date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJ_1gxMVif0x"
   },
   "outputs": [],
   "source": [
    "#Inspect tweets\n",
    "display(trumpTweets.head(10))\n",
    "display(trumpTweets.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmhUUsf0jnO_"
   },
   "source": [
    "#Extract Emoji's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMcLK_Q3ii-V"
   },
   "outputs": [],
   "source": [
    "!pip3 install -U emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EU8XptT4gN0_"
   },
   "outputs": [],
   "source": [
    "# Import the packages we need\n",
    "import emoji\n",
    "import regex\n",
    "\n",
    "# Define a function to get emojis\n",
    "'Function that extracts the emojis from a text and returns them in a list'\n",
    "def get_emojis(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI[\"en\"] for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KcSLoGOnKij"
   },
   "outputs": [],
   "source": [
    "# Let's do that for all of our tweets and store the emojis in a new column called \"emojis\"\n",
    "\n",
    "trumpTweets['emojis'] = trumpTweets['content'].apply(get_emojis)\n",
    "\n",
    "# Check if it worked\n",
    "display(trumpTweets.iloc[340:352,:]) # logical indexing: from tweets get rows 340,352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3D9dToJ5m7rA"
   },
   "source": [
    "#Remove Undesirable Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncoJbNS6m7rB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import regular expressions\n",
    "import re\n",
    "\n",
    "# Set-up patterns to be removed fro the tweets\n",
    "pat1 = r\"http\\S+\"\n",
    "pat2 = r\"#\"\n",
    "pat3 = r\"@\"\n",
    "pat4 = r\"FAV\"\n",
    "pat5 = r\"RE\"\n",
    "pat6 = r\"pic.\\S+\"\n",
    "pat7 = r\"\\n\"\n",
    "pat8 = '\\r\\n'\n",
    "pat9 = r'|'.join((r'&amp;',r'&copy;',r'&reg;',r'&quot;',r'&gt;',r'&lt;',r'&nbsp;',r'&apos;',r'&cent;',r'&euro;',r'&pound;'))\n",
    "# Combine all patterns\n",
    "combined_pat = r'|'.join((pat1, pat2, pat3, pat4, pat5, pat6, pat7, pat8, pat9))\n",
    "\n",
    "# replace the patterns with an empty string\n",
    "trumpTweets['stripped'] =  [re.sub(combined_pat, '', w) for w in trumpTweets.content]\n",
    "# might have double spaces now (because of empty string replacements above) - remove double empty spaces\n",
    "trumpTweets['stripped']  = [re.sub(r\"  \", ' ', w) for w in trumpTweets.loc[:,'stripped']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yD3EsvHLm7rB"
   },
   "source": [
    "#Apply Sentiment Scores to Each Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Gad6h8m9Mzu"
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXpFXqnEm7rC"
   },
   "outputs": [],
   "source": [
    "# Import the sentiment module (if you haven't already done so)\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Import numpy (if you have not already done so)\n",
    "import numpy as np\n",
    "\n",
    "# Instantiate the sentiment analyzer (if you haven't already done so)\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Now get the compound sentiment score for each tweet\n",
    "trumpTweets['C_Score'] = np.nan\n",
    "for index, row in trumpTweets.iterrows(): \n",
    "    trumpTweets.loc[index, 'C_Score'] = analyser.polarity_scores(row['stripped'])['compound']\n",
    "# Let's take a look!\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cdfn6lmggucS"
   },
   "outputs": [],
   "source": [
    "trumpTweets[['stripped','C_Score']][540:552]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrSyJg4sm7rC"
   },
   "source": [
    "#Cumulative Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIVtEjyvm7rC"
   },
   "outputs": [],
   "source": [
    "# import necessary modules (if not already imported)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Count positive tweets: {sum(trumpTweets['C_Score'] > 0.05)}\")\n",
    "print(f\"Count netural tweets: {trumpTweets['C_Score'].between(-0.05, 0.05).sum()}\")\n",
    "print(f\"Count negative tweets: {sum(trumpTweets['C_Score'] < -0.05)}\")\n",
    "print(f\"Total number of tweets: {trumpTweets['C_Score'].count()}\")\n",
    "      \n",
    "display(trumpTweets.C_Score.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wN_aC0Ncm7rC"
   },
   "source": [
    "#Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJibwWWem7rC"
   },
   "outputs": [],
   "source": [
    "# import necessary modules (if not already imported)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# settings for seaborn plotting style\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# settings for seaborn plot sizes\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "\n",
    "# Create Histogram\n",
    "ax = sns.histplot(trumpTweets['C_Score'],\n",
    "                  bins=10,\n",
    "                  kde=False,\n",
    "                  color='skyblue')\n",
    "ax.set(xlabel='Sentiment Distribution', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rA_MQDLKm7rD"
   },
   "outputs": [],
   "source": [
    "# create new column\n",
    "trumpTweets['Sentiment'] = np.nan\n",
    "\n",
    "# Loop through rows of dataframe and determine strings for new column \"Sentiment\"\n",
    "for index, row in trumpTweets.iterrows(): \n",
    "    if trumpTweets.loc[index, 'C_Score'] > 0.05 : \n",
    "            trumpTweets.loc[index, 'Sentiment'] = \"Positive\"       \n",
    "    elif trumpTweets.loc[index, 'C_Score'] < -0.05 :\n",
    "            trumpTweets.loc[index, 'Sentiment'] = \"Negative\"   \n",
    "    else : \n",
    "        trumpTweets.loc[index, 'Sentiment'] = \"Neutral\"\n",
    "\n",
    "trumpTweets['Sentiment'] = trumpTweets['Sentiment'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blu9pC-zm7rD"
   },
   "outputs": [],
   "source": [
    "# See whether it worked \n",
    "trumpTweets[['stripped','C_Score', 'Sentiment']].tail(25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMpOr8BI-qwr"
   },
   "source": [
    "#Sentiment Distribution over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRL4xEh5m7rE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "trumpTweets['day'] = [one.date() for one in trumpTweets['date']]\n",
    "trumpTweets = trumpTweets.sort_values(by=['day'])\n",
    "\n",
    "sentiments = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "positiveProps = (trumpTweets[trumpTweets.Sentiment == 'Positive'].groupby(['day']).count()[['Sentiment']]/ trumpTweets.groupby(['day']).count()[['Sentiment']])*100\n",
    "neutralProps = (trumpTweets[trumpTweets.Sentiment == 'Neutral'].groupby(['day']).count()[['Sentiment']]/ trumpTweets.groupby(['day']).count()[['Sentiment']])*100\n",
    "negativeProps = (trumpTweets[trumpTweets.Sentiment == 'Negative'].groupby(['day']).count()[['Sentiment']]/ trumpTweets.groupby(['day']).count()[['Sentiment']])*100\n",
    " \n",
    "positiveProps = positiveProps['Sentiment'].tolist()\n",
    "neutralProps = neutralProps['Sentiment'].tolist()\n",
    "negativeProps = negativeProps['Sentiment'].tolist()\n",
    "plt.figure(figsize=[16, 5])\n",
    "barWidth = 0.5\n",
    "labels = trumpTweets.day.unique()\n",
    "r = np.arange(len(labels))\n",
    "positiveProps = [0 if math.isnan(x) else x for x in positiveProps]\n",
    "neutralProps = [0 if math.isnan(x) else x for x in neutralProps]\n",
    "negativeProps = [0 if math.isnan(x) else x for x in negativeProps]\n",
    "\n",
    "plt.bar(r,positiveProps, color='lightgreen', edgecolor='white', width=barWidth)\n",
    "plt.bar(r, neutralProps, bottom=positiveProps, color='skyblue', edgecolor='white', width=barWidth)\n",
    "plt.bar(r, negativeProps, bottom=[i+j for i,j in zip(positiveProps, neutralProps)], color='orange', edgecolor='white', width=barWidth)\n",
    " \n",
    "plt.xticks(r, labels, rotation = 45, fontsize=12)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.suptitle('Sentiment Distribution over Time')\n",
    "plt.xlabel(\"Date\", fontsize=18)\n",
    "plt.ylabel(\"Share\", fontsize=20)\n",
    "plt.legend(sentiments)\n",
    "plt.show()\n",
    "# Sort by Index again\n",
    "trumpTweets.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOmbvmla7cmn"
   },
   "source": [
    "#Tweet Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IrMFTay70Bc"
   },
   "outputs": [],
   "source": [
    "# 1. You need to install it first:\n",
    "!pip3 install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zKcwLA6L6-U"
   },
   "outputs": [],
   "source": [
    "# 2. Preprocess the tweets\n",
    "\n",
    "# Import the preprocessor\n",
    "import preprocessor as prepro\n",
    "\n",
    "# Set options to remove URL, Reserved word\n",
    "prepro.set_options(prepro.OPT.URL, prepro.OPT.RESERVED)\n",
    "\n",
    "# Let's do it for all tweets\n",
    "trumpTweets['text']  = trumpTweets['content'].apply(prepro.clean)\n",
    "\n",
    "# Check our work\n",
    "trumpTweets['text'].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yf9jp8HB0_Uz"
   },
   "outputs": [],
   "source": [
    "# 3. Fix some things the preprocessor missed\n",
    "\n",
    "# Replace html &amp; with and\n",
    "trumpTweets['text'] =  [re.sub(r'&amp;', 'and', w) for w in trumpTweets.text]\n",
    "\n",
    "# Remove other html entities \n",
    "htmlents = r'|'.join((r'&copy;',r'&reg;',r'&quot;',r'&gt;',r'&lt;',r'&nbsp;',r'&apos;',r'&cent;',r'&euro;',r'&pound;')) \n",
    "trumpTweets['text'] =  [re.sub(htmlents, '', w) for w in trumpTweets.text]\n",
    "\n",
    "# Notice that Hashtags and Mentions are still in the tweets. Sometimes, we may want to treat them as nouns (e.g., for a brand or person)\n",
    "# To do so, we need to remove the special characters # and @ using regular expressions\n",
    "\n",
    "# Set-up patterns to be removed fro the tweets\n",
    "htag = r'|'.join((r\"#\", r\"@\"))\n",
    "\n",
    "# Now, replace the patterns with an empty string\n",
    "trumpTweets['text'] =  [re.sub(htag, '', w) for w in trumpTweets.text]\n",
    "\n",
    "# We might have double spaces now (because of empty string replacements above) - remove double empty spaces\n",
    "#tweets['stripped']  = [re.sub(r\"  \", ' ', w) for w in tweets.loc[:,'stripped']]\n",
    "trumpTweets['text'] = trumpTweets.text.replace({' +':' '},regex=True)\n",
    "\n",
    "# Check our Work\n",
    "trumpTweets['text'].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2ApkCCnm7rK"
   },
   "source": [
    "#EDA Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHOYH-6nm7rK"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "\n",
    "# 1. Instantiate sentiment analyzer\n",
    "sid_obj = SentimentIntensityAnalyzer()\n",
    "key_list = \"pos neu neg compound overall_sentiment\".split(\" \")\n",
    "for i in key_list:\n",
    "  trumpTweets[i] = \"\"\n",
    "\n",
    "# 2. Analyze the tweets text row by row in our dataframe and fill in it's sentiment, polarity, and overall sentiment\n",
    "for index, row in trumpTweets.iterrows():\n",
    "    row_sentiment_dict = sid_obj.polarity_scores(row['text'])\n",
    "    \n",
    "    if row_sentiment_dict['compound'] >= 0.05 : \n",
    "        row_sentiment_dict['overall_sentiment']  = \"Positive\"\n",
    "\n",
    "    elif row_sentiment_dict['compound'] <= - 0.05 : \n",
    "        row_sentiment_dict['overall_sentiment'] =  \"Negative\"\n",
    "\n",
    "    else : \n",
    "        row_sentiment_dict['overall_sentiment'] =  \"Neutral\"\n",
    "    for i in key_list:\n",
    "        trumpTweets.loc[index, i] = row_sentiment_dict[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsME7n6Zm7rK"
   },
   "outputs": [],
   "source": [
    "# 3. Let's check the overall sentiment distribution\n",
    "trumpTweets['overall_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uWOSXp2m7rN"
   },
   "outputs": [],
   "source": [
    "# Create List of Competitors\n",
    "Competitors = trumpTweets['query_text'].unique()\n",
    "\n",
    "# Create Stacked Bar Chart\n",
    "import matplotlib.pyplot as plt\n",
    "r = range(len(Competitors))\n",
    "sentiments = [\"positive\", \"neutral\", \"negative\"]\n",
    "\n",
    "plt.figure(figsize=[10, 5])\n",
    "\n",
    "positiveProps = (trumpTweets[trumpTweets.overall_sentiment == 'Positive'].groupby(['query_text']).count()[['overall_sentiment']]/ trumpTweets.groupby(['query_text']).count()[['overall_sentiment']])*100\n",
    "neutralProps = (trumpTweets[trumpTweets.overall_sentiment == 'Neutral'].groupby(['query_text']).count()[['overall_sentiment']]/ trumpTweets.groupby(['query_text']).count()[['overall_sentiment']])*100\n",
    "negativeProps = (trumpTweets[trumpTweets.overall_sentiment == 'Negative'].groupby(['query_text']).count()[['overall_sentiment']]/ trumpTweets.groupby(['query_text']).count()[['overall_sentiment']])*100\n",
    " \n",
    "positiveProps = positiveProps['overall_sentiment'].tolist()\n",
    "neutralProps = neutralProps['overall_sentiment'].tolist()\n",
    "negativeProps = negativeProps['overall_sentiment'].tolist()\n",
    "\n",
    "barWidth = 0.5\n",
    "labels = trumpTweets.groupby(['query_text']).query_text.unique().keys()\n",
    "plt.bar(r,positiveProps, color='#b5ffb9', edgecolor='white', width=barWidth)\n",
    "plt.bar(r, neutralProps, bottom=positiveProps, color='#FFFF99', edgecolor='white', width=barWidth)\n",
    "plt.bar(r, negativeProps, bottom=[i+j for i,j in zip(positiveProps, neutralProps)], color='#F08080', edgecolor='white', width=barWidth)\n",
    " \n",
    "plt.xticks(r, labels, rotation = 45, fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.suptitle('Distribution of Sentiment By Competitor')\n",
    "plt.xlabel(\"Competitor\", fontsize=18)\n",
    "plt.ylabel(\"Share of Tweets\", fontsize=20)\n",
    "plt.legend(sentiments)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_qU-sy3z6Dj"
   },
   "source": [
    "# Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OijcVnHOm7rN"
   },
   "outputs": [],
   "source": [
    "'Function to generate Word Cloud object'\n",
    "def plotWorldCloud(keyword):\n",
    "    df = trumpTweets[trumpTweets['query_text'] == keyword]\n",
    "    all_words = ' '.join([text for text in df[df['overall_sentiment'] == 'Positive']['text']])\n",
    "    if len(all_words) > 5:\n",
    "        wordcloud = WordCloud(collocations=True, width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "        return wordcloud\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# List of Competitors   \n",
    "Competitors = trumpTweets['query_text'].unique()\n",
    "\n",
    "# Build Grid\n",
    "gridSize = math.ceil(math.sqrt(len(Competitors)))\n",
    "\n",
    "fig = plt.figure(figsize=(30, 20))\n",
    "i = 0\n",
    "d={}\n",
    "for keyword in Competitors:\n",
    "    d[\"ax{0}\".format(i)]=fig.add_subplot(gridSize,gridSize,i+1)\n",
    "    wordcloud = plotWorldCloud(keyword)\n",
    "    if wordcloud:\n",
    "        d[\"ax{0}\".format(i)].title.set_text(keyword)\n",
    "        d[\"ax{0}\".format(i)].title.set_fontsize(30)\n",
    "        d[\"ax{0}\".format(i)].imshow(wordcloud)\n",
    "        d[\"ax{0}\".format(i)].axis('off')\n",
    "        i = i + 1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "TrumpSentiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
